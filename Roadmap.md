# üöÄ ROADMAP v2: –ü–ª–∞–Ω —Ä–∞–∑–≤–∏—Ç–∏—è Deep Diagnostic Bot

> –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è + —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —É–ª—É—á—à–µ–Ω–∏–π

---

## üìä –ê—É–¥–∏—Ç —Ç–µ–∫—É—â–µ–π —Å–∏—Å—Ç–µ–º—ã

### –ß—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ ‚úÖ

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –°—Ç–∞—Ç—É—Å | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |
|-----------|--------|-------------|
| –ë–∞–∑–æ–≤—ã–π flow | ‚úÖ | 10 –≤–æ–ø—Ä–æ—Å–æ–≤ ‚Üí –∞–Ω–∞–ª–∏–∑ ‚Üí –æ—Ç—á—ë—Ç |
| AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ | ‚úÖ | –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã —á–µ—Ä–µ–∑ Claude |
| –ê–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–æ–≤ | ‚ö†Ô∏è | –†–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ JSON parsing –Ω–µ—Å—Ç–∞–±–∏–ª–µ–Ω |
| –°–∫–æ—Ä–∏–Ω–≥ | ‚úÖ | 4 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á—ë—Ç |
| PDF —ç–∫—Å–ø–æ—Ä—Ç | ‚úÖ | –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –æ—Ç—á—ë—Ç |
| –ì–æ–ª–æ—Å–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è | ‚úÖ | Whisper —á–µ—Ä–µ–∑ RouterAI |
| Persistence | ‚úÖ | SQLite + SQLAlchemy |
| –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ | ‚úÖ | Middleware + —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ª–æ–≥–∏ |

### –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã üî¥

#### 1. JSON Parsing Failures (7 –∏–∑ 10 –æ—Ç–≤–µ—Ç–æ–≤)
```
ERROR - Failed to parse AI response as JSON: Extra data: line 21 column 1 (char 1598)
```

**–¢–µ–∫—É—â–∏–π –∫–æ–¥** (`src/ai/answer_analyzer.py:51-57`):
```python
clean_response = response.strip()
if clean_response.startswith("```"):
    lines = clean_response.split("\n")
    clean_response = "\n".join(lines[1:-1])

analysis = json.loads(clean_response)  # ‚Üê –ü–ê–î–ê–ï–¢
```

**–ü—Ä–æ–±–ª–µ–º–∞**: AI –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON + –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏/–æ–±—ä—è—Å–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ –Ω–µ–≥–æ.

**–†–µ—à–µ–Ω–∏–µ**:
```python
import re

def extract_json(text: str) -> dict:
    """–ò–∑–≤–ª–µ—á—å JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –º—É—Å–æ—Ä–æ–º."""
    # –°–ø–æ—Å–æ–± 1: JSONDecoder.raw_decode
    try:
        decoder = json.JSONDecoder()
        obj, _ = decoder.raw_decode(text.strip())
        return obj
    except json.JSONDecodeError:
        pass
    
    # –°–ø–æ—Å–æ–± 2: Regex –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON –æ–±—ä–µ–∫—Ç–∞
    match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text, re.DOTALL)
    if match:
        return json.loads(match.group())
    
    raise ValueError("No valid JSON found")
```

#### 2. –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã (25-90 —Å–µ–∫—É–Ω–¥)

**–ü—Ä–∏—á–∏–Ω—ã**:
- –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã: –∞–Ω–∞–ª–∏–∑ ‚Üí –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–∞
- –ë–æ–ª—å—à–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –∫–∞–∂–¥—ã–π —Ä–∞–∑
- –ù–µ—Ç streaming

**–¢–µ–∫—É—â–∏–π —Ç–∞–π–º–∏–Ω–≥ –ø–æ –ª–æ–≥–∞–º**:
| –û–ø–µ—Ä–∞—Ü–∏—è | –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è |
|----------|---------------|
| –ê–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–∞ | 18-26 —Å–µ–∫ |
| –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–∞ | 6-12 —Å–µ–∫ |
| –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ | 60-90 —Å–µ–∫ |
| **–ò—Ç–æ–≥–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å** | **25-35 —Å–µ–∫** |

**–†–µ—à–µ–Ω–∏—è**:
1. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–∞–Ω–∞–ª–∏–∑ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è)
2. Streaming –¥–ª—è UX
3. –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

#### 3. Fallback –Ω–∞ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏

–ü—Ä–∏ –æ—à–∏–±–∫–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ = 5. –≠—Ç–æ **–∏—Å–∫–∞–∂–∞–µ—Ç 70% —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤**.

```python
DEFAULT_ANALYSIS = {
    "scores": {
        "depth": 5,
        "self_awareness": 5,  # ‚Üê –°–µ—Ä–µ–¥–∏–Ω–∞ —à–∫–∞–ª—ã
        ...
    },
}
```

---

## üéØ –§–ê–ó–ê 1: –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è (3-5 –¥–Ω–µ–π)

### 1.1 –ò—Å–ø—Ä–∞–≤–∏—Ç—å JSON parsing

```python
# src/ai/answer_analyzer.py

import re
import json
from json import JSONDecoder

def robust_json_parse(text: str) -> dict:
    """
    –†–æ–±–∞—Å—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ AI.
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç:
    - JSON –≤ markdown –±–ª–æ–∫–∞—Ö
    - JSON —Å trailing text
    - JSON —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏
    """
    text = text.strip()
    
    # 1. –£–±–∏—Ä–∞–µ–º markdown code blocks
    if text.startswith("```"):
        # –ò—â–µ–º –∫–æ–Ω–µ—Ü –±–ª–æ–∫–∞
        end_idx = text.rfind("```")
        if end_idx > 3:
            text = text[text.find("\n")+1:end_idx]
    
    # 2. –ü—Ä–æ–±—É–µ–º raw_decode (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç trailing data)
    try:
        decoder = JSONDecoder()
        obj, idx = decoder.raw_decode(text)
        if isinstance(obj, dict):
            return obj
    except json.JSONDecodeError:
        pass
    
    # 3. Regex: –∏–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤—ã–π –≤–∞–ª–∏–¥–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç
    # –ò—â–µ–º { ... } —Å —É—á—ë—Ç–æ–º –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏
    brace_count = 0
    start_idx = None
    
    for i, char in enumerate(text):
        if char == '{':
            if brace_count == 0:
                start_idx = i
            brace_count += 1
        elif char == '}':
            brace_count -= 1
            if brace_count == 0 and start_idx is not None:
                try:
                    return json.loads(text[start_idx:i+1])
                except json.JSONDecodeError:
                    start_idx = None
    
    raise ValueError(f"No valid JSON found in: {text[:200]}...")


async def analyze_answer(question: str, answer: str, role: str) -> dict:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å —Ä–æ–±–∞—Å—Ç–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º."""
    try:
        messages = get_analysis_prompt(question, answer, role)
        response = await chat_completion(messages=messages, temperature=0.3, max_tokens=1000)
        
        analysis = robust_json_parse(response)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        if "scores" not in analysis:
            raise ValueError("Missing 'scores' in analysis")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π (0-10)
        for key, value in analysis["scores"].items():
            if not isinstance(value, (int, float)) or not 0 <= value <= 10:
                analysis["scores"][key] = 5  # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ
        
        return analysis
        
    except Exception as e:
        logger.error(f"Analysis failed: {e}, response: {response[:500] if 'response' in dir() else 'N/A'}")
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        return DEFAULT_ANALYSIS
```

### 1.2 –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ AI-–∑–∞–ø—Ä–æ—Å—ã

```python
# src/bot/handlers/diagnostic.py

import asyncio

async def process_answer(message: Message, state: FSMContext, bot: Bot):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏."""
    
    # ... –≤–∞–ª–∏–¥–∞—Ü–∏—è ...
    
    thinking_msg = await message.answer("üß† –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –æ—Ç–≤–µ—Ç...")
    
    # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û: –∞–Ω–∞–ª–∏–∑ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
    analysis_task = asyncio.create_task(
        analyze_answer(current_question, message.text, data["role"])
    )
    
    next_question_num = data["current_question"] + 1
    question_task = None
    
    if next_question_num <= TOTAL_QUESTIONS:
        # –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å–ª–µ–¥—É—é—â–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ —Å—Ä–∞–∑—É
        # (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –∏—Å—Ç–æ—Ä–∏—é, –∞–Ω–∞–ª–∏–∑ –¥–æ–±–∞–≤–∏–º –ø–æ—Ç–æ–º)
        question_task = asyncio.create_task(
            generate_question(
                role=data["role"],
                role_name=data["role_name"],
                experience=data["experience_name"],
                question_number=next_question_num,
                conversation_history=conversation_history,
                analysis_history=analysis_history,  # –ë–µ–∑ —Ç–µ–∫—É—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            )
        )
    
    # –ñ–¥—ë–º –∞–Ω–∞–ª–∏–∑
    analysis = await analysis_task
    analysis_history.append(analysis)
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å –∑–∞–¥–∞—á–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å ‚Äî –∂–¥—ë–º –µ—ë —Ç–æ–∂–µ
    if question_task:
        next_question = await question_task
    
    # –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: ~18-26 —Å–µ–∫ –≤–º–µ—Å—Ç–æ 25-35 —Å–µ–∫ (-30%)
```

### 1.3 Streaming –¥–ª—è UX

```python
# src/ai/client.py

async def chat_completion_stream(
    messages: list[dict],
    temperature: float = 0.7,
    max_tokens: int = 2000,
) -> AsyncGenerator[str, None]:
    """Streaming –≤–µ—Ä—Å–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è UX."""
    settings = get_settings()
    client = get_ai_client()
    
    stream = await client.chat.completions.create(
        model=settings.ai_model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
        stream=True,
    )
    
    async for chunk in stream:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content


# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç—á—ë—Ç–∞:
async def generate_report_with_progress(callback, ...):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ —Å live-–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º."""
    full_text = ""
    last_update = 0
    
    async for chunk in chat_completion_stream(messages, ...):
        full_text += chunk
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
        if len(full_text) - last_update > 500:
            await callback.message.edit_text(
                f"üìä –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç—á—ë—Ç...\n\n{full_text[:1000]}..."
            )
            last_update = len(full_text)
    
    return full_text
```

### 1.4 –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—ã—Ä—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ AI

```python
# –î–æ–±–∞–≤–∏—Ç—å –≤ analyze_answer –∏ –¥—Ä—É–≥–∏–µ AI-—Ñ—É–Ω–∫—Ü–∏–∏

import os
from datetime import datetime

DEBUG_LOG_DIR = "debug_logs"

def log_ai_response(prompt_type: str, response: str, success: bool):
    """–°–æ—Ö—Ä–∞–Ω—è—Ç—å —Å—ã—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã AI –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏."""
    os.makedirs(DEBUG_LOG_DIR, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    status = "ok" if success else "fail"
    filename = f"{DEBUG_LOG_DIR}/{timestamp}_{prompt_type}_{status}.txt"
    
    with open(filename, "w", encoding="utf-8") as f:
        f.write(response)
```

---

## üéØ –§–ê–ó–ê 2: –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ (1-2 –Ω–µ–¥–µ–ª–∏)

### 2.1 –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏

**–¢–µ–∫—É—â–∏–µ (5)**: depth, self_awareness, structure, honesty, expertise

**–ù–æ–≤—ã–µ (12)**:
```python
METRICS_V2 = {
    # –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ (Thinking 25%)
    "analytical_depth": "–ì–ª—É–±–∏–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–±–ª–µ–º",
    "systems_thinking": "–°–∏—Å—Ç–µ–º–Ω–æ–µ –≤–∏–¥–µ–Ω–∏–µ",
    "creativity": "–ù–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è",
    
    # –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ (Hard Skills 30%)
    "domain_expertise": "–≠–∫—Å–ø–µ—Ä—Ç–∏–∑–∞ –≤ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏",
    "methodology": "–í–ª–∞–¥–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è–º–∏",
    "tools_proficiency": "–í–ª–∞–¥–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏",
    
    # –ö–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω—ã–µ (Soft Skills 25%)
    "articulation": "–Ø—Å–Ω–æ—Å—Ç—å –∏–∑–ª–æ–∂–µ–Ω–∏—è",
    "empathy": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ç–µ–π–∫—Ö–æ–ª–¥–µ—Ä–æ–≤",
    "conflict_handling": "–†–∞–±–æ—Ç–∞ —Å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞–º–∏",
    
    # –õ–∏—á–Ω–æ—Å—Ç–Ω—ã–µ (Mindset 20%)
    "self_reflection": "–†–µ—Ñ–ª–µ–∫—Å–∏—è –∏ —Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫–∞",
    "growth_orientation": "–û—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ —Ä–æ—Å—Ç",
    "integrity": "–ß–µ—Å—Ç–Ω–æ—Å—Ç—å –∏ —ç—Ç–∏–∫–∞",
}
```

### 2.2 –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –æ—Ü–µ–Ω–æ–∫ –ø–æ –æ–ø—ã—Ç—É

```python
def calibrate_scores(scores: dict, experience: str, role: str) -> dict:
    """
    –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –æ—Ü–µ–Ω–æ–∫ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∑–∞—è–≤–ª–µ–Ω–Ω–æ–≥–æ –æ–ø—ã—Ç–∞.
    
    Junior —Å –æ—Ü–µ–Ω–∫–æ–π 7 –∑–∞ expertise ‚Äî —ç—Ç–æ —Ö–æ—Ä–æ—à–æ.
    Lead —Å –æ—Ü–µ–Ω–∫–æ–π 7 –∑–∞ expertise ‚Äî —ç—Ç–æ –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ.
    """
    experience_multipliers = {
        "junior": {"baseline": 4, "excellent_threshold": 6},
        "middle": {"baseline": 5, "excellent_threshold": 7},
        "senior": {"baseline": 6, "excellent_threshold": 8},
        "lead": {"baseline": 7, "excellent_threshold": 9},
    }
    
    calibrated = {}
    config = experience_multipliers.get(experience, experience_multipliers["middle"])
    
    for metric, value in scores.items():
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –æ–∂–∏–¥–∞–Ω–∏–π –¥–ª—è —É—Ä–æ–≤–Ω—è
        baseline = config["baseline"]
        if value >= config["excellent_threshold"]:
            calibrated[metric] = {"value": value, "assessment": "exceeds_expectations"}
        elif value >= baseline:
            calibrated[metric] = {"value": value, "assessment": "meets_expectations"}
        else:
            calibrated[metric] = {"value": value, "assessment": "below_expectations"}
    
    return calibrated
```

### 2.3 –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –≤–æ–ø—Ä–æ—Å–æ–≤

```python
# src/ai/question_gen.py

def get_question_difficulty(analysis_history: list[dict]) -> str:
    """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Å–ª–µ–¥—É—é—â–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞."""
    if not analysis_history:
        return "standard"
    
    # –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –æ—Ç–≤–µ—Ç–∞
    recent = analysis_history[-3:]
    avg_scores = []
    
    for analysis in recent:
        scores = analysis.get("scores", {})
        avg = sum(scores.values()) / len(scores) if scores else 5
        avg_scores.append(avg)
    
    overall_avg = sum(avg_scores) / len(avg_scores)
    
    if overall_avg >= 8:
        return "challenging"  # –ü—Ä–æ–≤–æ–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ, –≥–ª—É–±–∏–Ω–Ω—ã–µ
    elif overall_avg >= 6:
        return "standard"     # –û–±—ã—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
    else:
        return "supportive"   # –£–ø—Ä–æ—â—ë–Ω–Ω—ã–µ, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–µ


async def generate_question(...) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é."""
    difficulty = get_question_difficulty(analysis_history)
    
    difficulty_instructions = {
        "challenging": """
            –ó–∞–¥–∞–π –ü–†–û–í–û–ö–ê–¶–ò–û–ù–ù–´–ô –≤–æ–ø—Ä–æ—Å:
            - –ù–∞–º–µ—Ä–µ–Ω–Ω–æ —Å–æ–∑–¥–∞–π –¥–∏—Å–∫–æ–º—Ñ–æ—Ä—Ç
            - –ü–æ–ø—Ä–æ—Å–∏ –ø—Ä–∏–≤–µ—Å—Ç–∏ –ø—Ä–∏–º–µ—Ä –ü–†–û–í–ê–õ–ê
            - –°–ø—Ä–æ—Å–∏ –æ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è—Ö –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –æ—Ç–≤–µ—Ç–∞—Ö
            - –ü–æ—Å—Ç–∞–≤—å –ø–µ—Ä–µ–¥ —Å–ª–æ–∂–Ω—ã–º –≤—ã–±–æ—Ä–æ–º
        """,
        "standard": """
            –ó–∞–¥–∞–π –≤–æ–ø—Ä–æ—Å —Å—Ä–µ–¥–Ω–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏:
            - –ü–æ–ø—Ä–æ—Å–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä
            - –£–≥–ª—É–±–∏—Å—å –≤ –≤—ã—è–≤–ª–µ–Ω–Ω—ã–µ —Ç–µ–º—ã
        """,
        "supportive": """
            –ó–∞–¥–∞–π –ü–û–î–î–ï–†–ñ–ò–í–ê–Æ–©–ò–ô –≤–æ–ø—Ä–æ—Å:
            - –ü–æ–º–æ–≥–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç—É —Ä–∞—Å–∫—Ä—ã—Ç—å—Å—è
            - –°–ø—Ä–æ—Å–∏ –æ —Ç–æ–º, —á—Ç–æ –ø–æ–ª—É—á–∞–µ—Ç—Å—è —Ö–æ—Ä–æ—à–æ
            - –ò–∑–±–µ–≥–∞–π –¥–∞–≤–ª–µ–Ω–∏—è
        """,
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –≤ –ø—Ä–æ–º–ø—Ç
    messages = get_question_prompt(...)
    messages[0]["content"] += difficulty_instructions[difficulty]
    
    return await chat_completion(messages, ...)
```

### 2.4 –î–µ—Ç–µ–∫—Ü–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤

```python
# src/ai/pattern_detector.py

SUSPICIOUS_PATTERNS = {
    "rehearsed_answers": [
        r"–∫–∞–∫ —è —É–∂–µ –≥–æ–≤–æ—Ä–∏–ª",
        r"–æ–±—ã—á–Ω–æ –≤ —Ç–∞–∫–∏—Ö —Å–ª—É—á–∞—è—Ö",
        r"–ø–æ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ \w+",
        r"—Å–æ–≥–ª–∞—Å–Ω–æ best practices",
    ],
    "evasive": [
        r"—Å–ª–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å",
        r"—ç—Ç–æ –∑–∞–≤–∏—Å–∏—Ç",
        r"–ø–æ-—Ä–∞–∑–Ω–æ–º—É",
        r"–Ω–µ –ø–æ–º–Ω—é —Ç–æ—á–Ω–æ",
    ],
    "overconfident": [
        r"—è –≤—Å–µ–≥–¥–∞",
        r"—É –º–µ–Ω—è –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –±—ã–ª–æ –ø—Ä–æ–±–ª–µ–º",
        r"—è –ª—É—á—à–∏–π –≤",
        r"–≤—Å–µ –≥–æ–≤–æ—Ä—è—Ç —á—Ç–æ —è",
    ],
}

def detect_patterns(answer: str) -> list[str]:
    """–í—ã—è–≤–∏—Ç—å –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –æ—Ç–≤–µ—Ç–µ."""
    detected = []
    
    for pattern_type, patterns in SUSPICIOUS_PATTERNS.items():
        for pattern in patterns:
            if re.search(pattern, answer, re.IGNORECASE):
                detected.append(pattern_type)
                break
    
    return detected
```

---

## üéØ –§–ê–ó–ê 3: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ (2-3 –Ω–µ–¥–µ–ª–∏)

### 3.1 –ü—Ä–æ—Ñ–∏–ª—å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π

```python
# src/analytics/competency_profile.py

from dataclasses import dataclass
from typing import Optional

@dataclass
class CompetencyProfile:
    """–ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏."""
    
    # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    role: str
    experience: str
    total_score: int
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    hard_skills: dict[str, float]  # methodology, tools, domain
    soft_skills: dict[str, float]  # communication, empathy, conflict
    thinking: dict[str, float]     # analytical, systems, creative
    mindset: dict[str, float]      # growth, integrity, reflection
    
    # –¢–æ–ø-3 —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã
    strengths: list[str]
    
    # –¢–æ–ø-3 –∑–æ–Ω—ã —Ä–æ—Å—Ç–∞
    growth_areas: list[str]
    
    # –ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ñ–∏–ª—å
    thinking_style: str  # analytical / creative / strategic / tactical
    communication_style: str  # direct / diplomatic / avoiding
    risk_tolerance: str  # conservative / moderate / aggressive
    motivation_driver: str  # growth / recognition / stability / impact
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –±–µ–Ω—á–º–∞—Ä–∫–æ–º
    percentile: int  # 0-100, –ø–æ–∑–∏—Ü–∏—è —Å—Ä–µ–¥–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    development_plan: list[str]
    recommended_resources: list[dict]  # books, courses, etc.


def build_profile(session: DiagnosticSession) -> CompetencyProfile:
    """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ—Ñ–∏–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ—Å—Å–∏–∏."""
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –æ—Ü–µ–Ω–∫–∏ –∏–∑ analysis_history
    all_scores = aggregate_scores(session.analysis_history)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    thinking_style = detect_thinking_style(session.conversation_history)
    communication_style = detect_communication_style(session.conversation_history)
    
    # –ù–∞—Ö–æ–¥–∏–º —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∏ –∑–æ–Ω—ã —Ä–æ—Å—Ç–∞
    sorted_scores = sorted(all_scores.items(), key=lambda x: x[1], reverse=True)
    strengths = [s[0] for s in sorted_scores[:3]]
    growth_areas = [s[0] for s in sorted_scores[-3:]]
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    development_plan = generate_development_plan(growth_areas, session.role)
    resources = find_recommended_resources(growth_areas, session.role)
    
    return CompetencyProfile(
        role=session.role,
        experience=session.experience,
        total_score=session.total_score,
        hard_skills=extract_category_scores(all_scores, "hard"),
        soft_skills=extract_category_scores(all_scores, "soft"),
        thinking=extract_category_scores(all_scores, "thinking"),
        mindset=extract_category_scores(all_scores, "mindset"),
        strengths=strengths,
        growth_areas=growth_areas,
        thinking_style=thinking_style,
        communication_style=communication_style,
        risk_tolerance=detect_risk_tolerance(session.conversation_history),
        motivation_driver=detect_motivation(session.conversation_history),
        percentile=calculate_percentile(session),
        development_plan=development_plan,
        recommended_resources=resources,
    )
```

### 3.2 –ë–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥

```python
# src/analytics/benchmark.py

async def calculate_percentile(session: DiagnosticSession) -> int:
    """
    –í—ã—á–∏—Å–ª–∏—Ç—å –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å—Ä–µ–¥–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö.
    
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ:
    - –¢–æ–π –∂–µ —Ä–æ–ª–∏ (designer / product)
    - –¢–æ–º—É –∂–µ —É—Ä–æ–≤–Ω—é –æ–ø—ã—Ç–∞ (¬±1 —É—Ä–æ–≤–µ–Ω—å)
    """
    async with get_session() as db:
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Ö–æ–∂–∏–µ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ —Å–µ—Å—Å–∏–∏
        similar_sessions = await db.execute(
            select(DiagnosticSession)
            .where(
                DiagnosticSession.role == session.role,
                DiagnosticSession.status == "completed",
                DiagnosticSession.total_score.isnot(None),
            )
        )
        
        all_scores = [s.total_score for s in similar_sessions.scalars()]
        
        if len(all_scores) < 10:
            return 50  # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
        
        # –°—á–∏—Ç–∞–µ–º –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å
        below_count = sum(1 for s in all_scores if s < session.total_score)
        percentile = int((below_count / len(all_scores)) * 100)
        
        return percentile


def get_benchmark_insights(session: DiagnosticSession, percentile: int) -> list[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    insights = []
    
    if percentile >= 90:
        insights.append(f"üèÜ –¢—ã –≤ —Ç–æ–ø-10% {session.role_name}–æ–≤ —Å –æ–ø—ã—Ç–æ–º {session.experience_name}")
    elif percentile >= 75:
        insights.append(f"üí™ –¢—ã –æ–ø–µ—Ä–µ–∂–∞–µ—à—å 75% –∫–æ–ª–ª–µ–≥ –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏")
    elif percentile >= 50:
        insights.append(f"üìä –¢—ã –≤ –≤–µ—Ä—Ö–Ω–µ–π –ø–æ–ª–æ–≤–∏–Ω–µ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ —Ç–≤–æ–µ–≥–æ —É—Ä–æ–≤–Ω—è")
    else:
        insights.append(f"üìà –ï—Å—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —Ä–æ—Å—Ç–∞ ‚Äî —Ç—ã –º–æ–∂–µ—à—å –ø–æ–¥–Ω—è—Ç—å—Å—è –≤—ã—à–µ")
    
    return insights
```

### 3.3 –¢—Ä–µ–∫–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

```python
# src/analytics/progress.py

@dataclass
class ProgressReport:
    """–û—Ç—á—ë—Ç –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ –º–µ–∂–¥—É –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞–º–∏."""
    
    sessions_count: int
    first_date: datetime
    last_date: datetime
    
    # –î–∏–Ω–∞–º–∏–∫–∞ –æ–±—â–µ–≥–æ —Å–∫–æ—Ä–∞
    first_score: int
    current_score: int
    score_change: int
    score_trend: str  # "growing" / "stable" / "declining"
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    category_changes: dict[str, int]  # {"hard_skills": +5, ...}
    
    # –£–ª—É—á—à–∏–≤—à–∏–µ—Å—è –æ–±–ª–∞—Å—Ç–∏
    improved_areas: list[str]
    
    # –£—Ö—É–¥—à–∏–≤—à–∏–µ—Å—è –æ–±–ª–∞—Å—Ç–∏
    declined_areas: list[str]
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
    recommendation: str


async def get_progress_report(user_id: int) -> Optional[ProgressReport]:
    """–ü–æ–ª—É—á–∏—Ç—å –æ—Ç—á—ë—Ç –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    async with get_session() as db:
        sessions = await db.execute(
            select(DiagnosticSession)
            .where(
                DiagnosticSession.user_id == user_id,
                DiagnosticSession.status == "completed",
            )
            .order_by(DiagnosticSession.completed_at)
        )
        
        sessions_list = list(sessions.scalars())
        
        if len(sessions_list) < 2:
            return None  # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        
        first = sessions_list[0]
        last = sessions_list[-1]
        
        score_change = last.total_score - first.total_score
        
        if score_change > 5:
            trend = "growing"
            recommendation = "–û—Ç–ª–∏—á–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞! –ü—Ä–æ–¥–æ–ª–∂–∞–π –≤ —Ç–æ–º –∂–µ –¥—É—Ö–µ."
        elif score_change < -5:
            trend = "declining"
            recommendation = "–ó–∞–º–µ—Ç–Ω–æ —Å–Ω–∏–∂–µ–Ω–∏–µ. –†–µ–∫–æ–º–µ–Ω–¥—É—é —É–¥–µ–ª–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ —Ä–∞–∑–≤–∏—Ç–∏—é."
        else:
            trend = "stable"
            recommendation = "–°—Ç–∞–±–∏–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å. –ü–æ–ø—Ä–æ–±—É–π –≤—ã–π—Ç–∏ –∏–∑ –∑–æ–Ω—ã –∫–æ–º—Ñ–æ—Ä—Ç–∞."
        
        return ProgressReport(
            sessions_count=len(sessions_list),
            first_date=first.started_at,
            last_date=last.completed_at,
            first_score=first.total_score,
            current_score=last.total_score,
            score_change=score_change,
            score_trend=trend,
            category_changes=calculate_category_changes(first, last),
            improved_areas=find_improved_areas(first, last),
            declined_areas=find_declined_areas(first, last),
            recommendation=recommendation,
        )
```

---

## üéØ –§–ê–ó–ê 4: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ (3-4 –Ω–µ–¥–µ–ª–∏)

### 4.1 Webhook –¥–ª—è –≤–Ω–µ—à–Ω–∏—Ö —Å–∏—Å—Ç–µ–º

```python
# src/integrations/webhook.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI()

class DiagnosticResult(BaseModel):
    session_id: int
    user_telegram_id: int
    user_name: str
    role: str
    experience: str
    total_score: int
    scores: dict
    report_summary: str
    completed_at: str


@app.post("/webhook/{webhook_id}")
async def send_to_webhook(webhook_id: str, result: DiagnosticResult):
    """–û—Ç–ø—Ä–∞–≤–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤–æ –≤–Ω–µ—à–Ω—é—é —Å–∏—Å—Ç–µ–º—É."""
    
    # –ü–æ–ª—É—á–∞–µ–º URL webhook –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
    webhook_url = await get_webhook_url(webhook_id)
    
    async with httpx.AsyncClient() as client:
        response = await client.post(
            webhook_url,
            json=result.dict(),
            headers={"Content-Type": "application/json"},
            timeout=30,
        )
        
        if response.status_code != 200:
            raise HTTPException(500, f"Webhook failed: {response.text}")
    
    return {"status": "sent"}
```

### 4.2 Notion –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

```python
# src/integrations/notion.py

from notion_client import AsyncClient

class NotionExporter:
    def __init__(self, token: str, database_id: str):
        self.client = AsyncClient(auth=token)
        self.database_id = database_id
    
    async def export_session(self, session: DiagnosticSession, profile: CompetencyProfile):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ Notion –±–∞–∑—É."""
        
        page = await self.client.pages.create(
            parent={"database_id": self.database_id},
            properties={
                "Name": {"title": [{"text": {"content": f"{session.user.first_name} - {session.role_name}"}}]},
                "Score": {"number": session.total_score},
                "Role": {"select": {"name": session.role_name}},
                "Experience": {"select": {"name": session.experience_name}},
                "Level": {"select": {"name": profile.get_level()}},
                "Strengths": {"multi_select": [{"name": s} for s in profile.strengths]},
                "Growth Areas": {"multi_select": [{"name": g} for g in profile.growth_areas]},
                "Date": {"date": {"start": session.completed_at.isoformat()}},
                "Telegram": {"url": f"https://t.me/{session.user.username}"},
            },
            children=[
                {
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"text": {"content": session.report[:2000]}}]
                    }
                }
            ]
        )
        
        return page["id"]
```

### 4.3 Telegram Mini App

```
tg-bot/
‚îú‚îÄ‚îÄ miniapp/
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ QuestionCard.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ProgressBar.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CompetencyRadar.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ReportView.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hooks/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ useTelegram.ts
‚îÇ   ‚îî‚îÄ‚îÄ vite.config.ts
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Mini App:**
- –ë–æ–≥–∞—Ç—ã–π UI (–∞–Ω–∏–º–∞—Ü–∏–∏, –≥—Ä–∞—Ñ–∏–∫–∏)
- Radar chart –¥–ª—è –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π
- –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –æ—Ç—á—ë—Ç
- Sharing —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

---

## üéØ –§–ê–ó–ê 5: DevOps & –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (2-3 –Ω–µ–¥–µ–ª–∏)

### 5.1 Docker

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Å–∏—Å—Ç–µ–º—ã
RUN apt-get update && apt-get install -y \
    fonts-dejavu-core \
    && rm -rf /var/lib/apt/lists/*

# –ö–æ–ø–∏—Ä—É–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# –ö–æ–ø–∏—Ä—É–µ–º –∫–æ–¥
COPY src/ src/
COPY *.py .

# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
ENV PYTHONUNBUFFERED=1

CMD ["python", "-m", "src.bot.main"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  bot:
    build: .
    env_file: .env
    volumes:
      - ./data:/app/data
    restart: unless-stopped
    depends_on:
      - redis
  
  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data
    restart: unless-stopped

volumes:
  redis_data:
```

### 5.2 –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ PostgreSQL

```python
# src/core/config.py

class Settings(BaseSettings):
    # Database
    database_url: str = "sqlite+aiosqlite:///diagnostic_bot.db"
    
    # –î–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞:
    # database_url: str = "postgresql+asyncpg://user:pass@host:5432/dbname"
    
    # Redis
    redis_url: str = "redis://localhost:6379/0"
```

### 5.3 Redis –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è

```python
# src/cache/redis_cache.py

import redis.asyncio as redis
import json

class CacheManager:
    def __init__(self, redis_url: str):
        self.redis = redis.from_url(redis_url)
    
    async def cache_analysis(self, session_id: int, question_num: int, analysis: dict):
        """–ö—ç—à–∏—Ä–æ–≤–∞—Ç—å –∞–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–∞."""
        key = f"analysis:{session_id}:{question_num}"
        await self.redis.setex(key, 3600, json.dumps(analysis))
    
    async def get_cached_analysis(self, session_id: int, question_num: int) -> dict | None:
        """–ü–æ–ª—É—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑."""
        key = f"analysis:{session_id}:{question_num}"
        data = await self.redis.get(key)
        return json.loads(data) if data else None
    
    async def cache_question(self, context_hash: str, question: str):
        """–ö—ç—à–∏—Ä–æ–≤–∞—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å."""
        key = f"question:{context_hash}"
        await self.redis.setex(key, 1800, question)  # 30 –º–∏–Ω—É—Ç
```

---

## üìÖ –¢–∞–π–º–ª–∞–π–Ω

| –§–∞–∑–∞ | –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å | –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç | –°—Ç–∞—Ç—É—Å |
|------|-------------|-----------|--------|
| 1. –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è | 3-5 –¥–Ω–µ–π | üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π | Pending |
| 2. –ö–∞—á–µ—Å—Ç–≤–æ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ | 1-2 –Ω–µ–¥–µ–ª–∏ | üü° –í—ã—Å–æ–∫–∏–π | Pending |
| 3. –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ | 2-3 –Ω–µ–¥–µ–ª–∏ | üü¢ –°—Ä–µ–¥–Ω–∏–π | Pending |
| 4. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ | 3-4 –Ω–µ–¥–µ–ª–∏ | üîµ –ù–∏–∑–∫–∏–π | Pending |
| 5. DevOps | 2-3 –Ω–µ–¥–µ–ª–∏ | üü£ –ù–∏–∑–∫–∏–π | Pending |

**–û–±—â–∏–π —Å—Ä–æ–∫**: 8-12 –Ω–µ–¥–µ–ª—å

---

## üéØ Quick Wins (–º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–µ–≥–æ–¥–Ω—è)

1. **‚úÖ –ò—Å–ø—Ä–∞–≤–∏—Ç—å JSON parsing** ‚Üí +30% —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ü–µ–Ω–æ–∫
2. **‚úÖ –î–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—ã—Ä—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ AI** ‚Üí –æ—Ç–ª–∞–¥–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤
3. **‚úÖ Typing indicator –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏** ‚Üí –ª—É—á—à–∏–π UX
4. **‚úÖ Retry –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö AI** ‚Üí –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å
5. **‚úÖ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–∞** ‚Üí –∑–∞—â–∏—Ç–∞ –æ—Ç —Å–ø–∞–º–∞

---

## üìä –ú–µ—Ç—Ä–∏–∫–∏ —É—Å–ø–µ—Ö–∞

| –ú–µ—Ç—Ä–∏–∫–∞ | –¢–µ–∫—É—â–µ–µ | –¶–µ–ª—å | –ö–∞–∫ –∏–∑–º–µ—Ä—è—Ç—å |
|---------|---------|------|--------------|
| JSON Parse Success | ~30% | 100% | –õ–æ–≥–∏ –æ—à–∏–±–æ–∫ |
| Avg Response Time | 25-35 —Å–µ–∫ | <15 —Å–µ–∫ | Timing middleware |
| Completion Rate | ? | >80% | –ë–î: started vs completed |
| User Satisfaction | ? | NPS >50 | –û–ø—Ä–æ—Å –ø–æ—Å–ª–µ –æ—Ç—á—ë—Ç–∞ |
| Repeat Usage | ? | >20% | –ë–î: users —Å >1 session |

---

## üîó –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

- `Roadmap.md` ‚Äî –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è
- `README.md` ‚Äî –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞
- `requirements.txt` ‚Äî –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
